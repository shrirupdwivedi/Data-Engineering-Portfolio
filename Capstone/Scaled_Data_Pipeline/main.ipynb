{"nbformat_minor": 2, "cells": [{"execution_count": 14, "cell_type": "code", "source": "from pyspark.sql import *\nfrom pyspark.sql.functions import *\nimport logging\n\nspark = SparkSession.builder.master('local').appName('app').getOrCreate()\n\nclass pipeline:\n    \"\"\"\n    input: path of input data folder\n    output: path of output processed data folder\n    \"\"\"\n    def __init__(self,input,output):\n        \n        self.input = input\n        self.output = output\n\n\n    def business_transform(self):\n        \n        \"\"\"\n            Reads business dataset, cleans data and stores processed files in output destination\n        \"\"\"\n        logging.info('Reading business dataset')\n        df_b = spark.read.json(self.input+\"yelp_academic_dataset_business.json\")\n\n        #take RestaurantsPriceRange2 value from attributes column :\n        df_b = df_b.withColumn(\"price_range\", col(\"attributes\").getField(\"RestaurantsPriceRange2\"))\n        df_b = df_b.na.drop() #drop null values \n        df_b = df_b.filter(col(\"categories\").contains(\"Restaurants\")) #filter out non restaurant businesses\n        df_b = df_b.drop(\"attributes\") #drop attribute column\n        logging.info('Saving business dataset')\n        df_b.write.format(\"parquet\").mode(\"overwrite\").save(self.output + \"business.parquet\")\n\n    def review_transform(self):\n        \n        \"\"\"\n            Reads review dataset, cleans data and stores processed files in output destination\n        \"\"\"\n        logging.info('Reading Review Dataset')\n        df_r = spark.read.json(self.input+\"yelp_academic_dataset_review.json\")\n        df_r = df_r.na.drop()\n        logging.info('Saving review dataset')\n        df_r.write.format(\"parquet\").mode(\"overwrite\").save(self.output + \"review.parquet\")\n\n    def user_transform(self):\n\n        \"\"\"\n            Reads user dataset, cleans data and stores processed files in output destination\n        \"\"\"\n        logging.info('Reading user dataset')\n        df_u = spark.read.json(self.input+\"yelp_academic_dataset_user.json\")\n        df_u = df_u.na.drop()\n        logging.info('saving user dataset...')\n        df_u.write.format(\"parquet\").mode(\"overwrite\").save(self.output + \"user.parquet\")\n    \n    def checkin_transform(self):\n\n        \"\"\"\n            Reads checkin dataset, cleans data and stores processed files in output destination\n        \"\"\"\n\n        logging.info(\"Reading checkin dataset\")\n        df_c = spark.read.json(self.input+\"yelp_academic_dataset_checkin.json\")\n        df_c = df_c.withColumn(\"no_of_checkins\",size(split(col(\"date\"),\",\")))\n        df_c = df_c.drop(\"date\")\n        logging.info('saving checkin dataset')\n        df_c.write.format(\"parquet\").mode(\"overwrite\").save(self.output + \"checkin.parquet\")\n\n    def tip_transform(self):\n        \n        \"\"\"\n            Reads tip dataset, cleans data and stores processed files in output destination\n        \"\"\"\n        logging.info('Reading tip dataset')\n        df_t = spark.read.json(self.input+\"yelp_academic_dataset_tip.json\")\n        df_t = df_t.na.drop()\n        logging.info('saving tip dataset...')\n        df_t.write.format(\"parquet\").mode(\"overwrite\").save(self.output + \"tip.parquet\")\n\n\n\n\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 51.02783203125, "end_time": 1666146609428.129}}, "collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "class Main:\n    \"\"\"\n        Args:\n            source: path of Azure blob storage containing datasets\n            output: path of Azure blob storage to store processed parquet files after transforming+cleaning\n    \"\"\"\n    def __init__(self):\n        logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)\n        path = \"/HdiNotebooks/\"\n        ETL_data = pipeline(path, path)\n        ETL_data.business_transform()\n        ETL_data.review_transform()\n        ETL_data.user_transform()\n        ETL_data.checkin_transform()\n        ETL_data.tip_transform()\n\n\nMain()\n\n    ", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error was encountered:\nSession 0 did not reach idle status in time. Current status is busy.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 15325.237060546875, "end_time": 1666146700357.367}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}