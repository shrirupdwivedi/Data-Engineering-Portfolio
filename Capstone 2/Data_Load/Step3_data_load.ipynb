{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql import *\nfrom pyspark.sql.types import *", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1666202487840_0004</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-rupu.xqkwh140vrjufei1v2xqlpvg4d.ux.internal.cloudapp.net:8088/proxy/application_1666202487840_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-rupu.xqkwh140vrjufei1v2xqlpvg4d.ux.internal.cloudapp.net:30060/node/containerlogs/container_1666202487840_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 62.39111328125, "end_time": 1666208383718.864}}, "collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "import os\nimport json\nimport pandas as pd\nfrom pathlib import Path\nimport decimal\nimport logging", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3308.1640625, "end_time": 1666208387042.951}}, "collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "# A set of functions\n\ndef parse_csv(line):\n    \"\"\"\n   This function will parse \".txt\" file from blob storage.\n\n   since we are working with comma-separated values file so we want to return event object\n\n   :param line: A line of \".txt\" file in CSV format.\n   :return: Common event object.\n   \"\"\"\n    try:\n        record_type_pos = 2 # filter for \"Q\" or \"T\" value\n        record = line.split(\",\")\n        # Filter by record_type (T = Trade or Q = Quote):\n        if record[record_type_pos] == \"T\":\n            # Create event object by using values from record object and performing data type conversion\n            event = [record[0], # trade_dt\n                     record[1], # file_tm\n                     record[2], # event_type\n                     record[3], # symbol\n                     record[4], # event_tm\n                     int(record[5]), # event_seq_nb\n                     record[6],   # exchange\n                     decimal.Decimal(record[7]), # bid_pr\n                     int(record[8]), # bid_size\n                     None, None,\n                     \"T\"]\n            return event\n        elif record[record_type_pos] == \"Q\":\n            event = [record[0], \n                     record[1], \n                     record[2], \n                     record[3], \n                     record[4], \n                     int(record[5]), \n                     record[6],\n                     decimal.Decimal(record[7]), \n                     int(record[8]), \n                     decimal.Decimal(record[9]), # ask_pr\n                     int(record[10]),            # ask_size\n                     \"Q\"]\n            return event\n    except Exception as e:\n        # Return exception as \"Bad record\" and convert values to None preceding record_type == \"B\"\n        event = [None, None, None, None, None, None, None, None, None, None, None, \"B\"]\n        logging.error(\"Bad record\", e)\n        # print(f\"Bad record: {e}\")\n        return event\n    \ndef parse_json(line):\n    \"\"\"\n    This function will parse through each line in the JSON formatted \".txt\" file stored from blob storage.\n\n    :param line: Each line of \".txt\" file in JSON format.\n    :return: common_event() object\n    \"\"\"\n    try:\n        record = json.loads(line)\n        record_type = record[\"event_type\"]\n        # Parse records for each type and convert data type as necessary\n        # Filter by record_type (T = Trade or Q = Quote):\n        if record_type == \"T\":\n            # Create event object based and\n            event = [record[\"trade_dt\"], \n                     record[\"file_tm\"], \n                     record[\"event_type\"], \n                     record[\"symbol\"],\n                     record[\"event_tm\"], \n                     int(record[\"event_seq_nb\"]), \n                     record[\"exchange\"],\n                     decimal.Decimal(record[\"bid_pr\"]), \n                     int(record[\"bid_size\"]), \n                     None, None, # Try place None values\n                     \"T\"]\n            return event\n        elif record_type == \"Q\":\n            event = [record[\"trade_dt\"], \n                     record[\"file_tm\"], \n                     record[\"event_type\"], \n                     record[\"symbol\"],\n                     record[\"event_tm\"], \n                     int(record[\"event_seq_nb\"]), \n                     record[\"exchange\"],\n                     decimal.Decimal(record[\"bid_pr\"]), \n                     int(record[\"bid_size\"]), \n                     decimal.Decimal(record[\"ask_pr\"]),\n                     int(record[\"ask_size\"]), \n                     \"Q\"]\n            return event\n    except Exception as e:\n        # Return exception as \"Bad record\" and convert values to None preceding record_type == \"B\"\n        event = [None, None, None, None, None, None, None, None, None, None, None, \"B\"]\n        logging.error(\"Bad record\", e)\n        # print(f\"Bad record: {e}\")\n        return event", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 65.095947265625, "end_time": 1666208401250.106}}, "collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "# Initialize schema\n\ncommonEventSchema = StructType([\n            StructField(\"trade_dt\", StringType(), True),\n            StructField(\"file_tm\", StringType(), True),\n            StructField(\"record_type\", StringType(), True),\n            StructField(\"symbol\", StringType(), True),\n            StructField(\"event_tm\", StringType(), True),\n            StructField(\"event_seq_nb\", IntegerType(), True),\n            StructField(\"exchange\", StringType(), True),\n            StructField(\"bid_pr\", DecimalType(), True),\n            StructField(\"bid_size\", IntegerType(), True),\n            StructField(\"ask_pr\", DecimalType(), True),\n            StructField(\"ask_size\", IntegerType(), True),\n            StructField(\"partition\", StringType(), True)\n\n])", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 49.43603515625, "end_time": 1666208408903.841}}, "collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "key = 'xDxCsSmD37UlT2gVRhO1+KQ77zbiHlr4BdbAGMeITp/6YIPZeV+pNF/0OYYG8z05cTAvBPGOmia9+AStxVaM/A'\nstorage_name = 'rupuhdistorage'\ncontainer_name = 'rupu-2022-10-19t17-52-17-362z'\ncsv_dir_1 = '/HdiNotebooks/part-00000-5e4ced0a-66e2-442a-b020-347d0df4df8f-c000.txt'\ncsv_dir_2 = '/HdiNotebooks/part-00000-214fff0a-f408-466c-bb15-095cd8b648dc-c000.txt'\njson_dir_1 = '/HdiNotebooks/part-00000-c6c48831-3d45-4887-ba5f-82060885fc6c-c000.txt'\njson_dir_2 = '/HdiNotebooks/part-00000-092ec1db-39ab-4079-9580-f7c7b516a283-c000.txt'", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 49.716796875, "end_time": 1666208840910.763}}, "collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "# Create Spark Session\nspark = SparkSession.builder.master('local').appName('app').getOrCreate()\n\nspark.conf.set(\n        \"fs.azure.account.key.%s.blob.core.windows.net\" % (storage_name), \n         key\n)\n\nspark.conf.set(\"fs.azure.account.key.pipehdistorage.blob.core.windows.net\",\"UfE04fjAVTlo5NtelKegLyczL21tpz8AQmaY0Mo3UoCj5vTPmuIBzMIrJ9G7jGePhcmUHkRT5BiV+ASti/8VVg\" )\n\n\n# Initilize spark context\nsc = spark.sparkContext\n\n# Raw text files\nraw_csv_1 = sc.textFile(\"wasbs://%s@%s.blob.core.windows.net%s\" %( container_name, storage_name, csv_dir_1))\n\nraw_csv_2 = sc.textFile( \"wasbs://%s@%s.blob.core.windows.net%s\" %( container_name, storage_name, csv_dir_2))\n\nraw_json_1 = sc.textFile( \"wasbs://%s@%s.blob.core.windows.net%s\" %( container_name, storage_name, json_dir_1))\n\nraw_json_2 =  sc.textFile( \"wasbs://%s@%s.blob.core.windows.net%s\" %( container_name, storage_name, json_dir_2))\n\n# Parsed files\nparsed_csv1 = raw_csv_1.map(lambda line: parse_csv(line))\nparsed_csv2 = raw_csv_2.map(lambda line: parse_csv(line))\n\nparsed_json1= raw_json_1.map(lambda line: parse_json(line))\nparsed_json2= raw_json_2.map(lambda line: parse_json(line))\n\n# Create Data Frames\nspark_df1 = spark.createDataFrame(parsed_csv1, commonEventSchema)\nspark_df2 = spark.createDataFrame(parsed_csv2, commonEventSchema)\nspark_df3 = spark.createDataFrame(parsed_json1, commonEventSchema)\nspark_df4 = spark.createDataFrame(parsed_json2, commonEventSchema)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5301.0751953125, "end_time": 1666208876676.089}}, "collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "union_df = spark_df1.union(spark_df2)\\\n                    .union(spark_df3)\\\n                    .union(spark_df4)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 251.664794921875, "end_time": 1666208884317.13}}, "collapsed": true}}, {"execution_count": 14, "cell_type": "code", "source": "union_df.write.partitionBy(\"partition\").mode(\"overwrite\").format(\"parquet\").save(\"/HdiNotebooks/output_dir\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5304.73095703125, "end_time": 1666209471350.821}}, "collapsed": true}}, {"execution_count": 15, "cell_type": "code", "source": "trade_common = spark.read.parquet(\"output_dir/partition=T\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 754.2861328125, "end_time": 1666209565447.583}}, "collapsed": true}}, {"execution_count": 17, "cell_type": "code", "source": "quote_common = spark.read.parquet(\"output_dir/partition=Q\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 751.691162109375, "end_time": 1666209593528.606}}, "collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "trade = trade_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\", \"file_tm\", \"bid_pr\")\nquote = quote_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\", \"file_tm\", \"bid_pr\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 259.385986328125, "end_time": 1666209604243.354}}, "collapsed": true}}, {"execution_count": 19, "cell_type": "code", "source": "trade.show(30)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------+--------+--------------------+------------+--------------------+------+\n|  trade_dt|symbol|exchange|            event_tm|event_seq_nb|             file_tm|bid_pr|\n+----------+------+--------+--------------------+------------+--------------------+------+\n|2020-08-06|  SYMB|    NYSE|2020-08-06 16:57:...|          60|2020-08-06 09:30:...|    33|\n|2020-08-06|  SYMB|    NYSE|2020-08-06 18:06:...|          70|2020-08-06 09:30:...|    34|\n|2020-08-06|  SYMB|    NYSE|2020-08-06 19:21:...|          80|2020-08-06 09:30:...|    33|\n|2020-08-06|  SYMB|    NYSE|2020-08-06 20:36:...|          90|2020-08-06 09:30:...|    33|\n|2020-08-06|  SYMB|    NYSE|2020-08-06 21:46:...|         100|2020-08-06 09:30:...|    36|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 10:42:...|          10|2020-08-06 09:30:...|   157|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 11:52:...|          20|2020-08-06 09:30:...|   160|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 13:01:...|          30|2020-08-06 09:30:...|   158|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 14:15:...|          40|2020-08-06 09:30:...|   161|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 15:18:...|          50|2020-08-06 09:30:...|   158|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 16:24:...|          60|2020-08-06 09:30:...|   162|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 17:37:...|          70|2020-08-06 09:30:...|   161|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 19:00:...|          80|2020-08-06 09:30:...|   159|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 20:14:...|          90|2020-08-06 09:30:...|   158|\n|2020-08-06|  SYMC|    NYSE|2020-08-06 21:32:...|         100|2020-08-06 09:30:...|   159|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 10:37:...|          10|2020-08-05 09:30:...|    79|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 11:56:...|          20|2020-08-05 09:30:...|    76|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 13:09:...|          30|2020-08-05 09:30:...|    75|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 14:24:...|          40|2020-08-05 09:30:...|    78|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 15:31:...|          50|2020-08-05 09:30:...|    78|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 16:37:...|          60|2020-08-05 09:30:...|    79|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 17:49:...|          70|2020-08-05 09:30:...|    77|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 19:04:...|          80|2020-08-05 09:30:...|    75|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 20:21:...|          90|2020-08-05 09:30:...|    75|\n|2020-08-05|  SYMA|    NYSE|2020-08-05 21:30:...|         100|2020-08-05 09:30:...|    78|\n|2020-08-05|  SYMB|    NYSE|2020-08-05 10:43:...|          10|2020-08-05 09:30:...|    35|\n|2020-08-05|  SYMB|    NYSE|2020-08-05 12:02:...|          20|2020-08-05 09:30:...|    33|\n|2020-08-05|  SYMB|    NYSE|2020-08-05 13:10:...|          30|2020-08-05 09:30:...|    34|\n|2020-08-05|  SYMB|    NYSE|2020-08-05 14:26:...|          40|2020-08-05 09:30:...|    37|\n|2020-08-05|  SYMB|    NYSE|2020-08-05 15:31:...|          50|2020-08-05 09:30:...|    36|\n+----------+------+--------+--------------------+------------+--------------------+------+\nonly showing top 30 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.597900390625, "end_time": 1666209691057.457}}, "collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "quote.show(30)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+------+--------+--------------------+------------+--------------------+------+\n|  trade_dt|symbol|exchange|            event_tm|event_seq_nb|             file_tm|bid_pr|\n+----------+------+--------+--------------------+------------+--------------------+------+\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 09:38:...|           1|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 09:46:...|           2|2020-08-06 09:30:...|    77|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 09:52:...|           3|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 09:58:...|           4|2020-08-06 09:30:...|    76|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:07:...|           5|2020-08-06 09:30:...|    77|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:15:...|           6|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:22:...|           7|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:29:...|           8|2020-08-06 09:30:...|    76|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:35:...|           9|2020-08-06 09:30:...|    76|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:50:...|          11|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 10:59:...|          12|2020-08-06 09:30:...|    76|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:06:...|          13|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:15:...|          14|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:23:...|          15|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:32:...|          16|2020-08-06 09:30:...|    77|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:40:...|          17|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:49:...|          18|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 11:55:...|          19|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:07:...|          21|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:15:...|          22|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:23:...|          23|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:28:...|          24|2020-08-06 09:30:...|    78|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:33:...|          25|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:41:...|          26|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:48:...|          27|2020-08-06 09:30:...|    76|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 12:55:...|          28|2020-08-06 09:30:...|    75|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 13:00:...|          29|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 13:16:...|          31|2020-08-06 09:30:...|    77|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 13:24:...|          32|2020-08-06 09:30:...|    79|\n|2020-08-06|  SYMA|  NASDAQ|2020-08-06 13:33:...|          33|2020-08-06 09:30:...|    76|\n+----------+------+--------+--------------------+------------+--------------------+------+\nonly showing top 30 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 808.8359375, "end_time": 1666211616879.197}}, "collapsed": false}}, {"execution_count": 56, "cell_type": "code", "source": "from pyspark.sql import functions as func\ndef applyLatest(df):\n    \n    #grouping the dataset by trade date and then aggregating it by maximum arrival time\n    correct_df = df.groupBy(\"trade_dt\").agg(func.max(\"event_tm\").alias(\"max_arrival_time\"))\n    # return only the latest arrival time\n    return correct_df.select(\"trade_dt\").collect()[0][0]", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 44.045166015625, "end_time": 1666212453892.993}}, "collapsed": false}}, {"execution_count": 57, "cell_type": "code", "source": "trade_corrected = applyLatest(trade)\nquote_corrected = applyLatest(quote)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2356.9990234375, "end_time": 1666212456508.163}}, "collapsed": false}}, {"execution_count": 58, "cell_type": "code", "source": "trade_corrected", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'2020-08-06'"}], "metadata": {"cell_status": {"execute_time": {"duration": 33.718994140625, "end_time": 1666212460143.157}}, "collapsed": false}}, {"execution_count": 59, "cell_type": "code", "source": "quote_corrected", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'2020-08-06'"}], "metadata": {"cell_status": {"execute_time": {"duration": 36.44482421875, "end_time": 1666212460518.968}}, "collapsed": false}}, {"execution_count": 67, "cell_type": "code", "source": "trade.write.parquet(\"/HdiNotebooks/trade/trade_dt={}\".format(trade_corrected)) #formatting with corrected schema ", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2258.132080078125, "end_time": 1666213283934.848}}, "collapsed": true}}, {"execution_count": 68, "cell_type": "code", "source": "quote.write.parquet(\"/HdiNotebooks/quote/trade_dt={}\".format(quote_corrected))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2265.198974609375, "end_time": 1666213359065.004}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}